{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b76d814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5993604",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"stock_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9400d507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>RSI</th>\n",
       "      <th>MACD</th>\n",
       "      <th>Bollinger_Upper</th>\n",
       "      <th>Bollinger_Lower</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "      <th>GDP_Growth</th>\n",
       "      <th>Inflation_Rate</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.374639</td>\n",
       "      <td>0.374780</td>\n",
       "      <td>0.373510</td>\n",
       "      <td>0.378390</td>\n",
       "      <td>0.298909</td>\n",
       "      <td>0.847286</td>\n",
       "      <td>0.741715</td>\n",
       "      <td>0.367146</td>\n",
       "      <td>0.366420</td>\n",
       "      <td>0.877177</td>\n",
       "      <td>0.580868</td>\n",
       "      <td>0.038604</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.950982</td>\n",
       "      <td>0.937746</td>\n",
       "      <td>0.938422</td>\n",
       "      <td>0.946158</td>\n",
       "      <td>0.094805</td>\n",
       "      <td>0.494543</td>\n",
       "      <td>0.881343</td>\n",
       "      <td>0.938396</td>\n",
       "      <td>0.935640</td>\n",
       "      <td>0.907192</td>\n",
       "      <td>0.527044</td>\n",
       "      <td>0.108908</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.732198</td>\n",
       "      <td>0.719825</td>\n",
       "      <td>0.723644</td>\n",
       "      <td>0.723158</td>\n",
       "      <td>0.126348</td>\n",
       "      <td>0.195471</td>\n",
       "      <td>0.463179</td>\n",
       "      <td>0.710666</td>\n",
       "      <td>0.702300</td>\n",
       "      <td>0.378363</td>\n",
       "      <td>0.351052</td>\n",
       "      <td>0.432540</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.598823</td>\n",
       "      <td>0.599865</td>\n",
       "      <td>0.596973</td>\n",
       "      <td>0.605322</td>\n",
       "      <td>0.180662</td>\n",
       "      <td>0.736684</td>\n",
       "      <td>0.289076</td>\n",
       "      <td>0.593793</td>\n",
       "      <td>0.586936</td>\n",
       "      <td>0.231614</td>\n",
       "      <td>0.493274</td>\n",
       "      <td>0.946349</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.156053</td>\n",
       "      <td>0.163410</td>\n",
       "      <td>0.155891</td>\n",
       "      <td>0.166084</td>\n",
       "      <td>0.203646</td>\n",
       "      <td>0.418698</td>\n",
       "      <td>0.318761</td>\n",
       "      <td>0.164158</td>\n",
       "      <td>0.156355</td>\n",
       "      <td>0.191642</td>\n",
       "      <td>0.365116</td>\n",
       "      <td>0.074867</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Open     Close      High       Low    Volume       RSI      MACD  \\\n",
       "0  0.374639  0.374780  0.373510  0.378390  0.298909  0.847286  0.741715   \n",
       "1  0.950982  0.937746  0.938422  0.946158  0.094805  0.494543  0.881343   \n",
       "2  0.732198  0.719825  0.723644  0.723158  0.126348  0.195471  0.463179   \n",
       "3  0.598823  0.599865  0.596973  0.605322  0.180662  0.736684  0.289076   \n",
       "4  0.156053  0.163410  0.155891  0.166084  0.203646  0.418698  0.318761   \n",
       "\n",
       "   Bollinger_Upper  Bollinger_Lower  Sentiment_Score  GDP_Growth  \\\n",
       "0         0.367146         0.366420         0.877177    0.580868   \n",
       "1         0.938396         0.935640         0.907192    0.527044   \n",
       "2         0.710666         0.702300         0.378363    0.351052   \n",
       "3         0.593793         0.586936         0.231614    0.493274   \n",
       "4         0.164158         0.156355         0.191642    0.365116   \n",
       "\n",
       "   Inflation_Rate  Target  \n",
       "0        0.038604       0  \n",
       "1        0.108908       0  \n",
       "2        0.432540       0  \n",
       "3        0.946349       0  \n",
       "4        0.074867       0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a150ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0    Open            10000 non-null  float64\n",
      " 1   Close            10000 non-null  float64\n",
      " 2   High             10000 non-null  float64\n",
      " 3   Low              10000 non-null  float64\n",
      " 4   Volume           10000 non-null  float64\n",
      " 5   RSI              10000 non-null  float64\n",
      " 6   MACD             10000 non-null  float64\n",
      " 7   Bollinger_Upper  10000 non-null  float64\n",
      " 8   Bollinger_Lower  10000 non-null  float64\n",
      " 9   Sentiment_Score  10000 non-null  float64\n",
      " 10  GDP_Growth       10000 non-null  float64\n",
      " 11  Inflation_Rate   10000 non-null  float64\n",
      " 12  Target           10000 non-null  int64  \n",
      "dtypes: float64(12), int64(1)\n",
      "memory usage: 1015.8 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a39b2553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>RSI</th>\n",
       "      <th>MACD</th>\n",
       "      <th>Bollinger_Upper</th>\n",
       "      <th>Bollinger_Lower</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "      <th>GDP_Growth</th>\n",
       "      <th>Inflation_Rate</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.494293</td>\n",
       "      <td>0.495067</td>\n",
       "      <td>0.493072</td>\n",
       "      <td>0.497314</td>\n",
       "      <td>0.496869</td>\n",
       "      <td>0.503171</td>\n",
       "      <td>0.501809</td>\n",
       "      <td>0.496134</td>\n",
       "      <td>0.492211</td>\n",
       "      <td>0.495362</td>\n",
       "      <td>0.500334</td>\n",
       "      <td>0.501918</td>\n",
       "      <td>0.05950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.287715</td>\n",
       "      <td>0.281587</td>\n",
       "      <td>0.284019</td>\n",
       "      <td>0.283283</td>\n",
       "      <td>0.289297</td>\n",
       "      <td>0.288361</td>\n",
       "      <td>0.287825</td>\n",
       "      <td>0.276432</td>\n",
       "      <td>0.276758</td>\n",
       "      <td>0.287750</td>\n",
       "      <td>0.288366</td>\n",
       "      <td>0.290510</td>\n",
       "      <td>0.23657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.246390</td>\n",
       "      <td>0.251759</td>\n",
       "      <td>0.247605</td>\n",
       "      <td>0.252727</td>\n",
       "      <td>0.244266</td>\n",
       "      <td>0.256966</td>\n",
       "      <td>0.253762</td>\n",
       "      <td>0.256406</td>\n",
       "      <td>0.253212</td>\n",
       "      <td>0.244864</td>\n",
       "      <td>0.246618</td>\n",
       "      <td>0.249640</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.492662</td>\n",
       "      <td>0.492477</td>\n",
       "      <td>0.491295</td>\n",
       "      <td>0.495413</td>\n",
       "      <td>0.493722</td>\n",
       "      <td>0.506117</td>\n",
       "      <td>0.504232</td>\n",
       "      <td>0.493400</td>\n",
       "      <td>0.489197</td>\n",
       "      <td>0.496597</td>\n",
       "      <td>0.503139</td>\n",
       "      <td>0.502395</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.740212</td>\n",
       "      <td>0.736107</td>\n",
       "      <td>0.736170</td>\n",
       "      <td>0.739736</td>\n",
       "      <td>0.750494</td>\n",
       "      <td>0.753489</td>\n",
       "      <td>0.752636</td>\n",
       "      <td>0.732336</td>\n",
       "      <td>0.729294</td>\n",
       "      <td>0.742288</td>\n",
       "      <td>0.750093</td>\n",
       "      <td>0.756607</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Open         Close          High           Low        Volume  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean       0.494293      0.495067      0.493072      0.497314      0.496869   \n",
       "std        0.287715      0.281587      0.284019      0.283283      0.289297   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.246390      0.251759      0.247605      0.252727      0.244266   \n",
       "50%        0.492662      0.492477      0.491295      0.495413      0.493722   \n",
       "75%        0.740212      0.736107      0.736170      0.739736      0.750494   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                RSI          MACD  Bollinger_Upper  Bollinger_Lower  \\\n",
       "count  10000.000000  10000.000000     10000.000000     10000.000000   \n",
       "mean       0.503171      0.501809         0.496134         0.492211   \n",
       "std        0.288361      0.287825         0.276432         0.276758   \n",
       "min        0.000000      0.000000         0.000000         0.000000   \n",
       "25%        0.256966      0.253762         0.256406         0.253212   \n",
       "50%        0.506117      0.504232         0.493400         0.489197   \n",
       "75%        0.753489      0.752636         0.732336         0.729294   \n",
       "max        1.000000      1.000000         1.000000         1.000000   \n",
       "\n",
       "       Sentiment_Score    GDP_Growth  Inflation_Rate       Target  \n",
       "count     10000.000000  10000.000000    10000.000000  10000.00000  \n",
       "mean          0.495362      0.500334        0.501918      0.05950  \n",
       "std           0.287750      0.288366        0.290510      0.23657  \n",
       "min           0.000000      0.000000        0.000000      0.00000  \n",
       "25%           0.244864      0.246618        0.249640      0.00000  \n",
       "50%           0.496597      0.503139        0.502395      0.00000  \n",
       "75%           0.742288      0.750093        0.756607      0.00000  \n",
       "max           1.000000      1.000000        1.000000      1.00000  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d537c0d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Open              0\n",
       "Close              0\n",
       "High               0\n",
       "Low                0\n",
       "Volume             0\n",
       "RSI                0\n",
       "MACD               0\n",
       "Bollinger_Upper    0\n",
       "Bollinger_Lower    0\n",
       "Sentiment_Score    0\n",
       "GDP_Growth         0\n",
       "Inflation_Rate     0\n",
       "Target             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97228231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 398 rows\n",
      "Date range: 2024-09-19 23:59:59.999000+00:00 to 2025-10-21 23:59:59.999000+00:00\n",
      "\n",
      "Calculating technical indicators...\n",
      "Created 53 features\n",
      "\n",
      "Sample of engineered features:\n",
      "                           timestamp        close   returns     rsi_14  \\\n",
      "393 2025-10-17 23:59:59.999000+00:00  106467.7852 -0.015882  26.422675   \n",
      "394 2025-10-18 23:59:59.999000+00:00  107198.2669  0.006861  27.656860   \n",
      "395 2025-10-19 23:59:59.999000+00:00  108666.7115  0.013698  28.455538   \n",
      "396 2025-10-20 23:59:59.999000+00:00  110588.9302  0.017689  29.846478   \n",
      "397 2025-10-21 23:59:59.999000+00:00  108476.8870 -0.019098  30.891852   \n",
      "\n",
      "            macd        sma_10  target_direction  \n",
      "393 -1495.468195  113807.91947                 1  \n",
      "394 -1867.796516  112192.25913                 1  \n",
      "395 -2021.080429  110888.37205                 1  \n",
      "396 -1964.803024  110625.82847                 0  \n",
      "397 -2066.802201  110392.72924                 0  \n",
      "\n",
      "Creating sequences for LSTM...\n",
      "Sequence shape: (383, 10, 49)\n",
      "Number of features: 49\n",
      "\n",
      "Splitting data chronologically...\n",
      "Train set: 268 samples\n",
      "Val set: 57 samples\n",
      "Test set: 58 samples\n",
      "\n",
      "Saving processed data...\n",
      "\n",
      "✓ Feature engineering complete!\n",
      "✓ Ready for model training with 49 features\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_and_prepare_data(filepath):\n",
    "    \"\"\"Load and initial preparation of stock data\"\"\"\n",
    "    df = pd.read_csv(filepath, sep='\\t')\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']]\n",
    "    for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def calculate_technical_indicators(df):\n",
    "    \"\"\"Calculate technical indicators using ONLY past data\"\"\"\n",
    "    df = df.copy()\n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    df['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    df['price_range'] = df['high'] - df['low']\n",
    "    df['price_range_pct'] = (df['high'] - df['low']) / df['close']\n",
    "    df['gap'] = df['open'] - df['close'].shift(1)\n",
    "    df['gap_pct'] = (df['open'] - df['close'].shift(1)) / df['close'].shift(1)\n",
    "\n",
    "    # Simplify moving averages for small data\n",
    "    df['sma_5'] = df['close'].rolling(window=5, min_periods=1).mean()\n",
    "    df['sma_10'] = df['close'].rolling(window=10, min_periods=1).mean()\n",
    "    df['sma_20'] = df['close'].rolling(window=20, min_periods=1).mean()\n",
    "    df['ema_5'] = df['close'].ewm(span=5, adjust=False).mean()\n",
    "    df['ema_10'] = df['close'].ewm(span=10, adjust=False).mean()\n",
    "    df['ema_20'] = df['close'].ewm(span=20, adjust=False).mean()\n",
    "    df['price_to_sma20'] = (df['close'] - df['sma_20']) / df['sma_20']\n",
    "\n",
    "    # RSI\n",
    "    def calculate_rsi(data, period=14):\n",
    "        delta = data.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "        loss = loss.replace(0, 1e-9)  # prevent division by zero\n",
    "        rs = gain / loss\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        return rsi\n",
    "\n",
    "    df['rsi_14'] = calculate_rsi(df['close'], 14)\n",
    "    df['rsi_7'] = calculate_rsi(df['close'], 7)\n",
    "\n",
    "    # MACD\n",
    "    df['ema_12'] = df['close'].ewm(span=12, adjust=False).mean()\n",
    "    df['ema_26'] = df['close'].ewm(span=26, adjust=False).mean()\n",
    "    df['macd'] = df['ema_12'] - df['ema_26']\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()\n",
    "    df['macd_diff'] = df['macd'] - df['macd_signal']\n",
    "\n",
    "    # ROC and Momentum (shorter windows)\n",
    "    df['roc_3'] = ((df['close'] - df['close'].shift(3)) / df['close'].shift(3)) * 100\n",
    "    df['momentum_3'] = df['close'] - df['close'].shift(3)\n",
    "\n",
    "    # Volatility\n",
    "    df['volatility_5'] = df['returns'].rolling(window=5, min_periods=1).std()\n",
    "    df['volatility_10'] = df['returns'].rolling(window=10, min_periods=1).std()\n",
    "\n",
    "    # Bollinger Bands\n",
    "    df['bb_middle'] = df['close'].rolling(window=10, min_periods=1).mean()\n",
    "    bb_std = df['close'].rolling(window=10, min_periods=1).std()\n",
    "    df['bb_upper'] = df['bb_middle'] + (bb_std * 2)\n",
    "    df['bb_lower'] = df['bb_middle'] - (bb_std * 2)\n",
    "    df['bb_width'] = df['bb_upper'] - df['bb_lower']\n",
    "    df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])\n",
    "\n",
    "    # ATR\n",
    "    high_low = df['high'] - df['low']\n",
    "    high_close = np.abs(df['high'] - df['close'].shift())\n",
    "    low_close = np.abs(df['low'] - df['close'].shift())\n",
    "    ranges = pd.concat([high_low, high_close, low_close], axis=1)\n",
    "    true_range = np.max(ranges, axis=1)\n",
    "    df['atr_7'] = true_range.rolling(window=7, min_periods=1).mean()\n",
    "\n",
    "    # Volume-based\n",
    "    df['volume_sma_5'] = df['volume'].rolling(window=5, min_periods=1).mean()\n",
    "    df['volume_ratio'] = df['volume'] / (df['volume_sma_5'] + 1e-9)\n",
    "    df['obv'] = (np.sign(df['returns']) * df['volume']).fillna(0).cumsum()\n",
    "\n",
    "    # Lag features (reduce to 1–3 lags)\n",
    "    for lag in [1, 2, 3]:\n",
    "        df[f'close_lag_{lag}'] = df['close'].shift(lag)\n",
    "        df[f'returns_lag_{lag}'] = df['returns'].shift(lag)\n",
    "        df[f'volume_lag_{lag}'] = df['volume'].shift(lag)\n",
    "\n",
    "    # Time features\n",
    "    df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "\n",
    "    # Target\n",
    "    df['target_price'] = df['close'].shift(-1)\n",
    "    df['target_direction'] = (df['target_price'] > df['close']).astype(int)\n",
    "    df['target_return'] = (df['target_price'] - df['close']) / df['close']\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_sequences(df, lookback=10, features_to_use=None):\n",
    "    \"\"\"Create sequences for LSTM model\"\"\"\n",
    "    if features_to_use is None:\n",
    "        exclude_cols = ['timestamp', 'target_price', 'target_direction', 'target_return']\n",
    "        features_to_use = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "    df_clean = df.dropna()\n",
    "    X = df_clean[features_to_use].values\n",
    "    y_price = df_clean['target_price'].values\n",
    "    y_direction = df_clean['target_direction'].values\n",
    "    timestamps = df_clean['timestamp'].values\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    X_sequences, y_prices, y_directions, sequence_timestamps = [], [], [], []\n",
    "    for i in range(lookback, len(X_scaled)):\n",
    "        X_sequences.append(X_scaled[i-lookback:i])\n",
    "        y_prices.append(y_price[i])\n",
    "        y_directions.append(y_direction[i])\n",
    "        sequence_timestamps.append(timestamps[i])\n",
    "\n",
    "    X_sequences = np.array(X_sequences)\n",
    "    y_prices = np.array(y_prices)\n",
    "    y_directions = np.array(y_directions)\n",
    "\n",
    "    return X_sequences, y_prices, y_directions, sequence_timestamps, scaler, features_to_use\n",
    "\n",
    "def split_time_series(X, y_price, y_direction, timestamps, train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"Split data chronologically\"\"\"\n",
    "    n = len(X)\n",
    "    train_size = int(n * train_ratio)\n",
    "    val_size = int(n * val_ratio)\n",
    "\n",
    "    X_train = X[:train_size]\n",
    "    y_price_train = y_price[:train_size]\n",
    "    y_dir_train = y_direction[:train_size]\n",
    "    time_train = timestamps[:train_size]\n",
    "\n",
    "    X_val = X[train_size:train_size+val_size]\n",
    "    y_price_val = y_price[train_size:train_size+val_size]\n",
    "    y_dir_val = y_direction[train_size:train_size+val_size]\n",
    "    time_val = timestamps[train_size:train_size+val_size]\n",
    "\n",
    "    X_test = X[train_size+val_size:]\n",
    "    y_price_test = y_price[train_size+val_size:]\n",
    "    y_dir_test = y_direction[train_size+val_size:]\n",
    "    time_test = timestamps[train_size+val_size:]\n",
    "\n",
    "    print(f\"Train set: {len(X_train)} samples\")\n",
    "    print(f\"Val set: {len(X_val)} samples\")\n",
    "    print(f\"Test set: {len(X_test)} samples\")\n",
    "\n",
    "    return (X_train, y_price_train, y_dir_train, time_train,\n",
    "            X_val, y_price_val, y_dir_val, time_val,\n",
    "            X_test, y_price_test, y_dir_test, time_test)\n",
    "\n",
    "\n",
    "# ============ MAIN EXECUTION ============\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading data...\")\n",
    "    df = load_and_prepare_data(r\"C:\\Users\\user\\OneDrive\\Desktop\\ML\\InterIIT\\my-pathway-project\\Task 2\\Bitcoin_historical_data.csv.csv\")\n",
    "    print(f\"Loaded {len(df)} rows\")\n",
    "    print(f\"Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "\n",
    "    print(\"\\nCalculating technical indicators...\")\n",
    "    df = calculate_technical_indicators(df)\n",
    "    print(f\"Created {len(df.columns)} features\")\n",
    "\n",
    "    print(\"\\nSample of engineered features:\")\n",
    "    print(df[['timestamp', 'close', 'returns', 'rsi_14', 'macd', 'sma_10', 'target_direction']].tail())\n",
    "\n",
    "    print(\"\\nCreating sequences for LSTM...\")\n",
    "    lookback = 10\n",
    "    X_seq, y_prices, y_directions, seq_times, scaler, feature_names = create_sequences(df, lookback=lookback)\n",
    "\n",
    "    print(f\"Sequence shape: {X_seq.shape}\")\n",
    "    print(f\"Number of features: {len(feature_names)}\")\n",
    "\n",
    "    print(\"\\nSplitting data chronologically...\")\n",
    "    splits = split_time_series(X_seq, y_prices, y_directions, seq_times)\n",
    "    (X_train, y_price_train, y_dir_train, time_train,\n",
    "     X_val, y_price_val, y_dir_val, time_val,\n",
    "     X_test, y_price_test, y_dir_test, time_test) = splits\n",
    "\n",
    "    print(\"\\nSaving processed data...\")\n",
    "    np.savez('processed_data.npz',\n",
    "             X_train=X_train, y_price_train=y_price_train, y_dir_train=y_dir_train,\n",
    "             X_val=X_val, y_price_val=y_price_val, y_dir_val=y_dir_val,\n",
    "             X_test=X_test, y_price_test=y_price_test, y_dir_test=y_dir_test,\n",
    "             time_test=time_test, feature_names=feature_names)\n",
    "\n",
    "    print(\"\\n✓ Feature engineering complete!\")\n",
    "    print(f\"✓ Ready for model training with {X_train.shape[2]} features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f74d10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5263\n",
      "Time-series safe Test Accuracy: 0.4138\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13 14]\n",
      " [20 11]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.48      0.43        27\n",
      "           1       0.44      0.35      0.39        31\n",
      "\n",
      "    accuracy                           0.41        58\n",
      "   macro avg       0.42      0.42      0.41        58\n",
      "weighted avg       0.42      0.41      0.41        58\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# =================== PART 1: RANDOM FOREST ===================\n",
    "\n",
    "# Flatten sequences for RF (LSTM sequences are 3D: samples x lookback x features)\n",
    "X_train_rf = X_train.reshape(X_train.shape[0], -1)\n",
    "X_val_rf = X_val.reshape(X_val.shape[0], -1)\n",
    "X_test_rf = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "y_train_rf = y_dir_train\n",
    "y_val_rf = y_dir_val\n",
    "y_test_rf = y_dir_test\n",
    "\n",
    "# Initialize model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# ============ 1. Train on initial training set ============\n",
    "rf_model.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "# ============ 2. Evaluate on validation set ============\n",
    "y_val_pred = rf_model.predict(X_val_rf)\n",
    "val_acc = accuracy_score(y_val_rf, y_val_pred)\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# ============ 3. Time-series safe rolling prediction on test set ============\n",
    "y_pred_rolling = []\n",
    "\n",
    "# Start with training set\n",
    "X_train_rf_full = X_train_rf.copy()\n",
    "y_train_full = y_train_rf.copy()\n",
    "\n",
    "for i in range(X_test_rf.shape[0]):\n",
    "    # Train on all past data up to current test point\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_model.fit(X_train_rf_full, y_train_full)\n",
    "    \n",
    "    # Predict the current test sample\n",
    "    x_test_sample = X_test_rf[i].reshape(1, -1)\n",
    "    y_pred = rf_model.predict(x_test_sample)\n",
    "    y_pred_rolling.append(y_pred[0])\n",
    "    \n",
    "    # Include this test point into training for next iteration\n",
    "    X_train_rf_full = np.vstack([X_train_rf_full, x_test_sample])\n",
    "    y_train_full = np.append(y_train_full, y_dir_test[i])\n",
    "\n",
    "# ============ 4. Evaluate test set ============\n",
    "test_acc = accuracy_score(y_dir_test, y_pred_rolling)\n",
    "print(f\"Time-series safe Test Accuracy: {test_acc:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_dir_test, y_pred_rolling))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_dir_test, y_pred_rolling))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77caa383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Time-Series Evaluation for Random Forest...\n",
      "\n",
      "Split 1 | Train samples: 63 | Test samples: 63\n",
      "Accuracy: 0.5079\n",
      "Confusion Matrix:\n",
      " [[15 17]\n",
      " [14 17]]\n",
      "\n",
      "Split 2 | Train samples: 126 | Test samples: 63\n",
      "Accuracy: 0.5238\n",
      "Confusion Matrix:\n",
      " [[10 25]\n",
      " [ 5 23]]\n",
      "\n",
      "Split 3 | Train samples: 189 | Test samples: 63\n",
      "Accuracy: 0.5397\n",
      "Confusion Matrix:\n",
      " [[21  6]\n",
      " [23 13]]\n",
      "\n",
      "Split 4 | Train samples: 252 | Test samples: 63\n",
      "Accuracy: 0.5079\n",
      "Confusion Matrix:\n",
      " [[15 15]\n",
      " [16 17]]\n",
      "\n",
      "Split 5 | Train samples: 315 | Test samples: 63\n",
      "Accuracy: 0.5079\n",
      "Confusion Matrix:\n",
      " [[20 12]\n",
      " [19 12]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "def time_series_evaluation_rf(X, y, timestamps, n_splits=5):\n",
    "    \"\"\"\n",
    "    Perform time-series aware evaluation using expanding window.\n",
    "    Ensures no future data is used for training.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Features (2D array or 3D for sequences flattened)\n",
    "    - y: Target (binary or multi-class)\n",
    "    - timestamps: Corresponding timestamps\n",
    "    - n_splits: Number of splits for evaluation\n",
    "    \n",
    "    Returns:\n",
    "    - list of metrics for each split\n",
    "    \"\"\"\n",
    "    n_samples = len(X)\n",
    "    split_size = n_samples // (n_splits + 1)\n",
    "    results = []\n",
    "\n",
    "    for i in range(1, n_splits + 1):\n",
    "        train_end = split_size * i\n",
    "        test_end = split_size * (i + 1)\n",
    "\n",
    "        # Ensure test_end does not exceed length\n",
    "        if test_end > n_samples:\n",
    "            test_end = n_samples\n",
    "\n",
    "        X_train, X_test = X[:train_end], X[train_end:test_end]\n",
    "        y_train, y_test = y[:train_end], y[train_end:test_end]\n",
    "        \n",
    "        # Flatten sequences if needed\n",
    "        if len(X_train.shape) == 3:\n",
    "            X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "            X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "        else:\n",
    "            X_train_flat, X_test_flat = X_train, X_test\n",
    "\n",
    "        # Train Random Forest\n",
    "        rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "        rf.fit(X_train_flat, y_train)\n",
    "        y_pred = rf.predict(X_test_flat)\n",
    "\n",
    "        # Metrics\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        print(f\"\\nSplit {i} | Train samples: {len(X_train)} | Test samples: {len(X_test)}\")\n",
    "        print(f\"Accuracy: {acc:.4f}\")\n",
    "        print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "        results.append({\n",
    "            'split': i,\n",
    "            'train_size': len(X_train),\n",
    "            'test_size': len(X_test),\n",
    "            'accuracy': acc,\n",
    "            'report': report,\n",
    "            'confusion_matrix': cm\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# ================= MAIN EXECUTION =================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your processed sequences and targets from Step 1\n",
    "    data = np.load('processed_data.npz', allow_pickle=True)\n",
    "    X_train = data['X_train']\n",
    "    y_dir_train = data['y_dir_train']\n",
    "    X_val = data['X_val']\n",
    "    y_dir_val = data['y_dir_val']\n",
    "    X_test = data['X_test']\n",
    "    y_dir_test = data['y_dir_test']\n",
    "    time_test = data['time_test']\n",
    "\n",
    "    # Combine train + val for time-series evaluation\n",
    "    X_all = np.concatenate([X_train, X_val, X_test], axis=0)\n",
    "    y_all = np.concatenate([y_dir_train, y_dir_val, y_dir_test], axis=0)\n",
    "    timestamps_all = np.concatenate([data['X_train'], data['X_val'], data['X_test']], axis=0)\n",
    "\n",
    "    print(\"\\nStarting Time-Series Evaluation for Random Forest...\")\n",
    "    results = time_series_evaluation_rf(X_all, y_all, timestamps_all, n_splits=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0170a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features for LSTM: ['close_lag_1', 'ema_5', 'sma_10', 'macd_signal', 'bb_lower', 'rsi_14', 'roc_3', 'close_lag_3', 'ema_26', 'bb_position', 'volume_ratio', 'close', 'gap', 'month', 'ema_10', 'day_of_week', 'bb_middle', 'volume_lag_2']\n",
      "Train set: 268 samples\n",
      "Val set: 57 samples\n",
      "Test set: 58 samples\n",
      "Reduced feature set ready for LSTM: 18 features\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Fit Random Forest on flattened sequences\n",
    "rf = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)  # flatten time dimension\n",
    "rf.fit(X_train_flat, y_dir_train)\n",
    "\n",
    "# Step 2: Get feature importances\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Flattened feature names corresponding to X_train_flat\n",
    "feature_names_flat = []\n",
    "lookback = X_train.shape[1]\n",
    "for f in feature_names:  # feature_names from your LSTM feature list\n",
    "    feature_names_flat.extend([f + f\"_t-{i}\" for i in range(lookback, 0, -1)])\n",
    "\n",
    "# Create a sorted list of features by importance\n",
    "feat_imp_df = pd.DataFrame({\n",
    "    'feature': feature_names_flat,\n",
    "    'importance': importances\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Step 3: Select top N features (e.g., top 20)\n",
    "top_features_flat = feat_imp_df['feature'].head(20).tolist()\n",
    "\n",
    "# Map back to original base features for LSTM input\n",
    "top_base_features = list({f.split(\"_t-\")[0] for f in top_features_flat})\n",
    "print(\"Selected features for LSTM:\", top_base_features)\n",
    "\n",
    "# Step 4: Re-create sequences using only top features\n",
    "X_seq_top, y_prices_top, y_directions_top, seq_times_top, scaler_top, feature_names_top = create_sequences(\n",
    "    df, lookback=X_train.shape[1], features_to_use=top_base_features\n",
    ")\n",
    "\n",
    "# Step 5: Split again chronologically\n",
    "splits_top = split_time_series(X_seq_top, y_prices_top, y_directions_top, seq_times_top)\n",
    "(X_train_top, y_price_train_top, y_dir_train_top, time_train_top,\n",
    " X_val_top, y_price_val_top, y_dir_val_top, time_val_top,\n",
    " X_test_top, y_price_test_top, y_dir_test_top, time_test_top) = splits_top\n",
    "\n",
    "print(f\"Reduced feature set ready for LSTM: {len(top_base_features)} features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64f6afdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 31ms/step - accuracy: 0.5149 - loss: 0.6938 - val_accuracy: 0.5263 - val_loss: 0.6878\n",
      "Epoch 2/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5000 - loss: 0.6937 - val_accuracy: 0.5614 - val_loss: 0.6876\n",
      "Epoch 3/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4963 - loss: 0.6949 - val_accuracy: 0.5614 - val_loss: 0.6876\n",
      "Epoch 4/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4701 - loss: 0.6998 - val_accuracy: 0.6140 - val_loss: 0.6890\n",
      "Epoch 5/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5299 - loss: 0.6930 - val_accuracy: 0.5263 - val_loss: 0.6878\n",
      "Epoch 6/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5373 - loss: 0.6934 - val_accuracy: 0.5263 - val_loss: 0.6880\n",
      "Epoch 7/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5522 - loss: 0.6882 - val_accuracy: 0.5263 - val_loss: 0.6881\n",
      "Epoch 8/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5224 - loss: 0.6905 - val_accuracy: 0.5263 - val_loss: 0.6878\n",
      "Epoch 9/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5522 - loss: 0.6888 - val_accuracy: 0.5439 - val_loss: 0.6884\n",
      "Epoch 10/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5187 - loss: 0.6922 - val_accuracy: 0.5263 - val_loss: 0.6881\n",
      "Epoch 11/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5261 - loss: 0.6891 - val_accuracy: 0.5263 - val_loss: 0.6884\n",
      "Epoch 12/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5373 - loss: 0.6876 - val_accuracy: 0.5614 - val_loss: 0.6890\n",
      "Epoch 13/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5373 - loss: 0.6901 - val_accuracy: 0.5263 - val_loss: 0.6890\n",
      "Epoch 13: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# ==========================\n",
    "# CONFIGURATION\n",
    "# ==========================\n",
    "lookback = 20  # strictly past 20 rows\n",
    "n_features = X_train_top.shape[2]  # number of features\n",
    "batch_size = 8  # smaller batch for tiny data\n",
    "\n",
    "# ==========================\n",
    "# MODEL\n",
    "# ==========================\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "\n",
    "lookback = X_train_top.shape[1]\n",
    "n_features = X_train_top.shape[2]\n",
    "\n",
    "model = Sequential([\n",
    "    GRU(16, return_sequences=True, input_shape=(lookback, n_features)),\n",
    "    Dropout(0.2),\n",
    "    GRU(8),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_top, y_dir_train_top,\n",
    "    validation_data=(X_val_top, y_dir_val_top),\n",
    "    epochs=100,\n",
    "    batch_size=8,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8797438c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">880</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,088</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m18\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m)    │        \u001b[38;5;34m880\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m)    │         \u001b[38;5;34m64\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m)    │      \u001b[38;5;34m1,088\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │        \u001b[38;5;34m272\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m17\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,321</span> (9.07 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,321\u001b[0m (9.07 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,289</span> (8.94 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,289\u001b[0m (8.94 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> (128.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m32\u001b[0m (128.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - accuracy: 0.4888 - loss: 0.7168 - val_accuracy: 0.4737 - val_loss: 0.7032\n",
      "Epoch 2/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5299 - loss: 0.6982 - val_accuracy: 0.4386 - val_loss: 0.7020\n",
      "Epoch 3/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5075 - loss: 0.6966 - val_accuracy: 0.4035 - val_loss: 0.6991\n",
      "Epoch 4/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5896 - loss: 0.6758 - val_accuracy: 0.4211 - val_loss: 0.6977\n",
      "Epoch 5/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5336 - loss: 0.6868 - val_accuracy: 0.4561 - val_loss: 0.6956\n",
      "Epoch 6/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5448 - loss: 0.6793 - val_accuracy: 0.4561 - val_loss: 0.6977\n",
      "Epoch 7/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5709 - loss: 0.6688 - val_accuracy: 0.5263 - val_loss: 0.6965\n",
      "Epoch 8/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5597 - loss: 0.6842 - val_accuracy: 0.5088 - val_loss: 0.6972\n",
      "Epoch 9/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5448 - loss: 0.6852 - val_accuracy: 0.5088 - val_loss: 0.6979\n",
      "Epoch 10/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5634 - loss: 0.6913 - val_accuracy: 0.5088 - val_loss: 0.6931\n",
      "Epoch 11/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5560 - loss: 0.6855 - val_accuracy: 0.5439 - val_loss: 0.6987\n",
      "Epoch 12/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5299 - loss: 0.6918 - val_accuracy: 0.4737 - val_loss: 0.7174\n",
      "Epoch 13/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5410 - loss: 0.6912 - val_accuracy: 0.4386 - val_loss: 0.7052\n",
      "Epoch 14/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5448 - loss: 0.6872 - val_accuracy: 0.4737 - val_loss: 0.6980\n",
      "Epoch 15/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5709 - loss: 0.6820 - val_accuracy: 0.4737 - val_loss: 0.6999\n",
      "Epoch 16/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5299 - loss: 0.6877 - val_accuracy: 0.5263 - val_loss: 0.7031\n",
      "Epoch 17/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5336 - loss: 0.6803 - val_accuracy: 0.5263 - val_loss: 0.6996\n",
      "Epoch 18/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5522 - loss: 0.6861 - val_accuracy: 0.5263 - val_loss: 0.6965\n",
      "Epoch 19/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5933 - loss: 0.6736 - val_accuracy: 0.5263 - val_loss: 0.6969\n",
      "Epoch 20/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5672 - loss: 0.6808 - val_accuracy: 0.5263 - val_loss: 0.6995\n",
      "Epoch 20: early stopping\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "\n",
      "Test Accuracy: 0.5172\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step\n",
      "Confusion Matrix:\n",
      " [[19  8]\n",
      " [20 11]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4872    0.7037    0.5758        27\n",
      "           1     0.5789    0.3548    0.4400        31\n",
      "\n",
      "    accuracy                         0.5172        58\n",
      "   macro avg     0.5331    0.5293    0.5079        58\n",
      "weighted avg     0.5362    0.5172    0.5032        58\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Dropout, BatchNormalization, GlobalMaxPooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# ==========================\n",
    "# CONFIGURATION\n",
    "# ==========================\n",
    "lookback = X_train_top.shape[1]  # sequence length\n",
    "n_features = X_train_top.shape[2]  # number of features\n",
    "\n",
    "# ==========================\n",
    "# MODEL\n",
    "# ==========================\n",
    "from tensorflow.keras.layers import Input, Conv1D, Dense, Dropout, BatchNormalization, GlobalAveragePooling1D, MultiHeadAttention\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "inputs = Input(shape=(lookback, n_features))\n",
    "\n",
    "x = Conv1D(16, kernel_size=3, activation='relu', padding='causal')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# tiny attention\n",
    "attn_output = MultiHeadAttention(num_heads=2, key_dim=8)(x, x)\n",
    "x = x + attn_output  # residual\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "x = Dense(16, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# CALLBACKS\n",
    "# ==========================\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# TRAINING\n",
    "# ==========================\n",
    "history = model.fit(\n",
    "    X_train_top, y_dir_train_top,\n",
    "    validation_data=(X_val_top, y_dir_val_top),\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# EVALUATION\n",
    "# ==========================\n",
    "test_loss, test_acc = model.evaluate(X_test_top, y_dir_test_top, verbose=0)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# ==========================\n",
    "# PREDICTIONS\n",
    "# ==========================\n",
    "y_pred_prob = model.predict(X_test_top)\n",
    "y_pred_dir = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# ==========================\n",
    "# CONFUSION MATRIX & REPORT\n",
    "# ==========================\n",
    "cm = confusion_matrix(y_dir_test_top, y_pred_dir)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "report = classification_report(y_dir_test_top, y_pred_dir, digits=4)\n",
    "print(\"\\nClassification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d30b50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
